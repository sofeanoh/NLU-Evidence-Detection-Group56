{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EVIDENCE DETECTION : SOLUTION B (*Deep learning-based approaches that do not employ transformer architectures*)"
      ],
      "metadata": {
        "id": "dgSLpM8Ks87S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oXHaMg4Yt0lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRAINING ON TRAIN DATASET**"
      ],
      "metadata": {
        "id": "FjenhvahtUdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is used to train the model."
      ],
      "metadata": {
        "id": "6dVBytCR_Ugp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJu61so5fF0U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "dev_data = pd.read_csv('dev.csv')\n",
        "\n",
        "# Prepare the data - concatenate 'Claim' and 'Evidence' as the input text\n",
        "X_train = train_data['Claim'] + \" \" + train_data['Evidence']\n",
        "y_train = train_data['label']\n"
      ],
      "metadata": {
        "id": "AwNgfD86GSF0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000\n",
        "\n",
        "# Tokenizer fitting\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Save the tokenizer to a file\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n"
      ],
      "metadata": {
        "id": "EW11jVdHlYF3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b7kTXclnfbra"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to a fixed length\n",
        "max_len = 100  #this cover 100% of the dataset\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Bidirectional LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Single output node for binary classification\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ax2utU8srC8k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_data = pd.read_csv('dev.csv')\n",
        "X_dev = dev_data['Claim'] + \" \" + dev_data['Evidence']\n",
        "y_dev = dev_data['label']\n",
        "\n",
        "# Tokenize and pad sequences for the development data\n",
        "# use the tokenizer that was fit on the training data\n",
        "X_dev_seq = tokenizer.texts_to_sequences(X_dev)\n",
        "X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_len)  # Ensure max_len is the same as for the training data"
      ],
      "metadata": {
        "id": "hd8iGsBUJoHW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model on the entire training dataset\n",
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=64, validation_data=(X_dev_pad, y_dev), callbacks=[early_stopping])\n",
        "\n",
        "# Save the model after training\n",
        "model.save('bidirectional_lstm_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COoA1e_vGmlT",
        "outputId": "e4d44361-2c02-4ac6-f062-ee76b0e134cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "371/371 [==============================] - 168s 441ms/step - loss: 0.4649 - accuracy: 0.7804 - val_loss: 0.4070 - val_accuracy: 0.8068\n",
            "Epoch 2/10\n",
            "371/371 [==============================] - 173s 466ms/step - loss: 0.3401 - accuracy: 0.8504 - val_loss: 0.4613 - val_accuracy: 0.7892\n",
            "Epoch 3/10\n",
            "371/371 [==============================] - 175s 472ms/step - loss: 0.2639 - accuracy: 0.8912 - val_loss: 0.4750 - val_accuracy: 0.8005\n",
            "Epoch 4/10\n",
            "371/371 [==============================] - 173s 466ms/step - loss: 0.1997 - accuracy: 0.9214 - val_loss: 0.5390 - val_accuracy: 0.7886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iAYOFVSRtylJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EVALUATION ON DEVELOPMENT DATASET**"
      ],
      "metadata": {
        "id": "D9jVfX52jd6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code should be run for evaluation on the entire development dataset. They can be run independently."
      ],
      "metadata": {
        "id": "bc2sXWPdjiyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the saved model\n",
        "model_loaded = load_model('bidirectional_lstm_model.h5')\n",
        "# load development dataset\n",
        "dev_data = pd.read_csv('dev.csv')\n",
        "X_dev = dev_data['Claim'] + \" \" + dev_data['Evidence']\n",
        "y_dev = dev_data['label']\n",
        "\n",
        "# Tokenize and pad sequences for the development data\n",
        "# use the tokenizer that was fit on the training data\n",
        "# Load the tokenizer from a file\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "max_len = 100  #this cover 100% of the dataset\n",
        "\n",
        "X_dev_seq = tokenizer.texts_to_sequences(X_dev)\n",
        "X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_len)  # Ensure max_len is the same as for the training data\n",
        "\n",
        "\n",
        "# Evaluate the model on the development/validation set\n",
        "dev_loss, dev_accuracy = model_loaded.evaluate(X_dev_pad, y_dev, verbose=1)\n",
        "print(f\"Development Set Loss: {dev_loss}\")\n",
        "print(f\"Development Set Accuracy: {dev_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "# Make predictions on the development set\n",
        "dev_predictions = (model_loaded.predict(X_dev_pad) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(y_dev, dev_predictions)\n",
        "f1 = f1_score(y_dev, dev_predictions, average='weighted')\n",
        "precision = precision_score(y_dev, dev_predictions, average='weighted')\n",
        "recall = recall_score(y_dev, dev_predictions, average='weighted')\n",
        "\n",
        "print(f\"Accuracy on the development set: {accuracy:.4f}\")\n",
        "print(f\"F1 Score on the development set: {f1:.4f}\")\n",
        "print(f\"Precision on the development set: {precision:.4f}\")\n",
        "print(f\"Recall on the development set: {recall:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_dev, dev_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2etkGyw1ND4h",
        "outputId": "c41f0910-4b93-40c5-8130-34b60d763c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "186/186 [==============================] - 22s 106ms/step - loss: 0.4062 - accuracy: 0.8102\n",
            "Development Set Loss: 0.4061630666255951\n",
            "Development Set Accuracy: 81.02%\n",
            "186/186 [==============================] - 22s 112ms/step\n",
            "Accuracy on the development set: 0.8102\n",
            "F1 Score on the development set: 0.8060\n",
            "Precision on the development set: 0.8040\n",
            "Recall on the development set: 0.8102\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87      4327\n",
            "           1       0.67      0.59      0.62      1599\n",
            "\n",
            "    accuracy                           0.81      5926\n",
            "   macro avg       0.76      0.74      0.75      5926\n",
            "weighted avg       0.80      0.81      0.81      5926\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Esqx9VvBtw7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PRE-TESTING ON TRIAL DATASET**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W3fH9qyOjuGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is intended to further evaluate the model using a trial dataset. They can also be run independently."
      ],
      "metadata": {
        "id": "jGaxMX3SEdnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the saved model\n",
        "model_loaded = load_model('bidirectional_lstm_model.h5')\n",
        "# load trial dataset\n",
        "trial_data = pd.read_csv('trial.csv')\n",
        "X_trial = trial_data['Claim'] + \" \" + trial_data['Evidence']\n",
        "y_trial = trial_data['label']\n",
        "\n",
        "# Tokenize and pad sequences for the trial data\n",
        "# use the tokenizer that was fit on the training data\n",
        "# Load the tokenizer from a file\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "max_len = 100  #this cover 100% of the dataset\n",
        "\n",
        "X_trial_seq = tokenizer.texts_to_sequences(X_trial)\n",
        "X_trial_pad = pad_sequences(X_trial_seq, maxlen=max_len)  # Ensure max_len is the same as for the training data\n",
        "\n",
        "\n",
        "# Evaluate the model on the terial set\n",
        "trial_loss, trial_accuracy = model_loaded.evaluate(X_trial_pad, y_trial, verbose=1)\n",
        "print(f\"Trial Set Loss: {trial_loss}\")\n",
        "print(f\"Trial Set Accuracy: {trial_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "# Make predictions on the trial set\n",
        "trial_predictions = (model_loaded.predict(X_trial_pad) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(y_trial, trial_predictions)\n",
        "f1 = f1_score(y_trial, trial_predictions, average='weighted')\n",
        "precision = precision_score(y_trial, trial_predictions, average='weighted')\n",
        "recall = recall_score(y_trial, trial_predictions, average='weighted')\n",
        "\n",
        "print(f\"Accuracy on the Trial set: {accuracy:.4f}\")\n",
        "print(f\"F1 Score on the Trial set: {f1:.4f}\")\n",
        "print(f\"Precision on the Trial set: {precision:.4f}\")\n",
        "print(f\"Recall on the Trial set: {recall:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_trial, trial_predictions))"
      ],
      "metadata": {
        "id": "YS3-azW-hjCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b234ae-3862-4dfc-a946-f0448ec9450b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 43ms/step - loss: 0.2758 - accuracy: 0.9000\n",
            "Trial Set Loss: 0.27575358748435974\n",
            "Trial Set Accuracy: 90.00%\n",
            "2/2 [==============================] - 1s 39ms/step\n",
            "Accuracy on the Trial set: 0.9000\n",
            "F1 Score on the Trial set: 0.8990\n",
            "Precision on the Trial set: 0.8988\n",
            "Recall on the Trial set: 0.9000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93        35\n",
            "           1       0.86      0.80      0.83        15\n",
            "\n",
            "    accuracy                           0.90        50\n",
            "   macro avg       0.89      0.87      0.88        50\n",
            "weighted avg       0.90      0.90      0.90        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "t6VguGLUtu49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEMO CODE**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MgNmvxJZmnCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is used to produce the prediction file."
      ],
      "metadata": {
        "id": "ex5PPyTqswga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Load the saved model\n",
        "model_loaded = load_model('bidirectional_lstm_model.h5')\n",
        "\n",
        "# Load the test dataset\n",
        "# To test with other dataset, upload the file into the environment first, and then replace 'test.csv' with other .csv file\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Ensure that 'Claim' and 'Evidence' columns are of string type\n",
        "test_data['Claim'] = test_data['Claim'].astype(str)\n",
        "test_data['Evidence'] = test_data['Evidence'].astype(str)\n",
        "\n",
        "# Combine 'Claim' and 'Evidence' columns into a single text\n",
        "X_test = test_data['Claim'] + \" \" + test_data['Evidence']\n",
        "\n",
        "# Load the tokenizer from a file\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Define max_len as the same value used during training\n",
        "max_len = 100\n",
        "\n",
        "# Tokenize and pad sequences for the test data\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = model_loaded.predict(X_test_pad).round().astype(int)\n",
        "\n",
        "# Create a DataFrame with the predictions\n",
        "predictions_df = pd.DataFrame(test_predictions, columns=['prediction'])\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "predictions_df.to_csv('Group_56_B.csv', index=False)\n",
        "\n",
        "print(\"Predictions have been written to Group_56_B.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYxte0NNmmuB",
        "outputId": "590344a2-bef3-4f0d-ce54-df57c41054b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147/147 [==============================] - 18s 111ms/step\n",
            "Predictions have been written to Group_56_B.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}