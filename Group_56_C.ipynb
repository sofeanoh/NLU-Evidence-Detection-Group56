{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "\n",
        "### Import necessary libraries\n",
        "\n",
        "### Load the training dataset\n",
        "\n",
        "### Preprocess the data\n",
        "- Initialize a BERT tokenizer to convert text into tokens (small pieces of text) that the model can understand.\n",
        "\n",
        "### Prepare data for training\n",
        "\n",
        "- Create a DataLoader for the training dataset, using a RandomSampler to shuffle the data (helps in learning and generalization).\n",
        "\n",
        "### Set up the model for sequence classification\n",
        "- Load a pre-trained BERT model configured for sequence classification tasks.\n",
        "- Specify the number of labels for the classification (2 for binary classification).to determine the class probabilities.\n",
        "\n",
        "### Prepare the model for training\n",
        "- Set up an optimizer (AdamW) for adjusting model parameters and a learning rate scheduler to adjust the learning rate over time.\n",
        "\n",
        "### Define a function to train the model for one epoch\n",
        "    - Switch the BERT model to training mode.\n",
        "    - Initialize a variable to keep track of the total training loss.\n",
        "    - For each batch of data:\n",
        "        - Move the batch to the appropriate device.\n",
        "        - Clear any previously calculated gradients.\n",
        "        - Perform a forward pass through the model with the current batch of data.\n",
        "        -Extract embeddings from the BERT model's last hidden layer\n",
        "        - Calculate the loss (difference between model predictions and actual labels).\n",
        "        - Accumulate this batch's loss.\n",
        "        - Perform backpropagation to calculate gradients.\n",
        "        - Clip gradients to prevent excessively large updates.\n",
        "        - Update model parameters using the optimizer.\n",
        "        - Update the learning rate using the scheduler.\n",
        "    - Calculate the average loss over all batches.\n",
        "\n",
        "Train the Logistic Regression classifier using the scaled embeddings and corresponding labels\n",
        " on the logits from the BERT model.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "74nhMF57TK8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "#loading the datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "\n",
        "#preprocess the data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_length = 256\n",
        "\n",
        "def encode_data(df):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "    for _, row in df.iterrows():\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            row['Claim'] + ' [SEP] ' + row['Evidence'],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "        labels.append(row['label'])\n",
        "    labels = torch.tensor(labels)\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    return TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "#create DataLoader\n",
        "batch_size = 16\n",
        "train_dataset = encode_data(train_df)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "#setup the BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2,  # Binary classification\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=True  # Enable output of hidden states\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "#training setup\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 1\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        embeddings = outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()  # Extract [CLS] token embeddings\n",
        "        all_embeddings.extend(embeddings)\n",
        "        all_labels.extend(b_labels.detach().cpu().numpy())\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return total_train_loss / len(dataloader), np.array(all_embeddings), np.array(all_labels)\n",
        "\n",
        "#extract features and train Logistic Regression\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    train_loss, embeddings, labels = train_epoch(model, train_dataloader)\n",
        "    print(f'Train loss: {train_loss}')\n",
        "torch.save(model.state_dict(), \"bert_for_sequence_classification.pth\")\n",
        "\n",
        "#initialize and train the Logistic Regression classifier\n",
        "scaler = StandardScaler()\n",
        "embeddings_scaled = scaler.fit_transform(embeddings)\n",
        "lr_classifier = LogisticRegression(max_iter=1000)\n",
        "lr_classifier.fit(embeddings_scaled, labels)\n",
        "\n",
        "#save the trained BERT, Logistic Regression, and scaler\n",
        "\n",
        "joblib.dump(lr_classifier, 'lr_classifier.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "\n",
        "print(\"Training complete. All models saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L5dwdfXHW5Z",
        "outputId": "a26ea3bf-abe8-4922-c322-5ea30dbf3627"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1482/1482 [08:16<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.34335434407890647\n",
            "Training complete. All models saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation\n",
        "####Load the evaluation (development) dataset\n",
        "####Preprocess the evaluation data\n",
        "- Use the previously defined `encode_data` function to:\n",
        "    - Tokenize and encode each data point (a pair of text sequences) in the evaluation dataset.\n",
        "    - Generate attention masks for the sequences.\n",
        "    - Collect these into a TensorDataset object for efficient handling.\n",
        "\n",
        "#### Prepare the evaluation dataset for processing\n",
        "\n",
        "#### Define a function to evaluate the model on the dataset\n",
        "- The function takes a model and a DataLoader as inputs.\n",
        "- Switch the model to evaluation mode using `model.eval()` to inform the model that it is being evaluated, not trained. This disables dropout and batch normalization.\n",
        "\n",
        "- Perform a forward pass through the model to get the output logits for the batch.\n",
        "   \n",
        "- Convert these logits to actual predictions using `torch.argmax()`, which selects the index (class label) with the highest logit value.\n",
        "\n"
      ],
      "metadata": {
        "id": "jcWrQjTjxB3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define function to encode data\n",
        "def encode_data(df, tokenizer, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "    for _, row in df.iterrows():\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            row['Claim'] + ' [SEP] ' + row['Evidence'],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "        labels.append(row['label'])\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Evaluation function that incorporates the Logistic Regression classifier\n",
        "def evaluate_model(model, lr_classifier, scaler, dataloader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_hidden_states=True)\n",
        "\n",
        "        # Extract the [CLS] token embeddings from the last hidden layer\n",
        "        embeddings = outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n",
        "        # Scale embeddings\n",
        "        scaled_embeddings = scaler.transform(embeddings)\n",
        "        # Logistic Regression predictions\n",
        "        batch_predictions = lr_classifier.predict(scaled_embeddings)\n",
        "\n",
        "        predictions.extend(batch_predictions)\n",
        "        true_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "# Load saved models and scaler\n",
        "lr_classifier = joblib.load('lr_classifier.joblib')\n",
        "scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "# Load data\n",
        "eval_df = pd.read_csv('dev.csv')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_length = 256\n",
        "eval_dataset = encode_data(eval_df, tokenizer, max_length)\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    sampler=SequentialSampler(eval_dataset),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# Load the BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.load_state_dict(torch.load('bert_for_sequence_classification.pth', map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate the model using Logistic Regression\n",
        "eval_accuracy, eval_report = evaluate_model(model, lr_classifier, scaler, eval_dataloader)\n",
        "print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "print(eval_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hInqy10PXic",
        "outputId": "598c1120-e68d-4886-d798-c1e902c8f21a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|██████████| 371/371 [00:43<00:00,  8.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8655079311508607\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.92      0.89      0.91      4327\n",
            "     Class 1       0.73      0.79      0.76      1599\n",
            "\n",
            "    accuracy                           0.87      5926\n",
            "   macro avg       0.83      0.84      0.83      5926\n",
            "weighted avg       0.87      0.87      0.87      5926\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "#initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#define function to encode data\n",
        "def encode_data(df, tokenizer, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "    for _, row in df.iterrows():\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            row['Claim'] + ' [SEP] ' + row['Evidence'],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "        labels.append(row['label'])\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "#evaluation function that incorporates the Logistic Regression classifier\n",
        "def evaluate_model(model, lr_classifier, scaler, dataloader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_hidden_states=True)\n",
        "\n",
        "        # Extract the [CLS] token embeddings from the last hidden layer\n",
        "        embeddings = outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n",
        "        # Scale embeddings\n",
        "        scaled_embeddings = scaler.transform(embeddings)\n",
        "        # Logistic Regression predictions\n",
        "        batch_predictions = lr_classifier.predict(scaled_embeddings)\n",
        "\n",
        "        predictions.extend(batch_predictions)\n",
        "        true_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "#load saved models and scaler\n",
        "lr_classifier = joblib.load('lr_classifier.joblib')\n",
        "scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "#load data\n",
        "eval_df = pd.read_csv('ED_trial.csv')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_length = 256\n",
        "eval_dataset = encode_data(eval_df, tokenizer, max_length)\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    sampler=SequentialSampler(eval_dataset),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# Load the BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.load_state_dict(torch.load('bert_for_sequence_classification.pth', map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate the model using Logistic Regression\n",
        "eval_accuracy, eval_report = evaluate_model(model, lr_classifier, scaler, eval_dataloader)\n",
        "print(f\"Testing Accuracy: {eval_accuracy}\")\n",
        "print(eval_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrmegdRxpb9W",
        "outputId": "f2dfe330-0d84-41ad-81be-fb23370c184b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 11.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.97      0.89      0.93        35\n",
            "     Class 1       0.78      0.93      0.85        15\n",
            "\n",
            "    accuracy                           0.90        50\n",
            "   macro avg       0.87      0.91      0.89        50\n",
            "weighted avg       0.91      0.90      0.90        50\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo code\n",
        "Replace the input file\n",
        "###Ensure necessary packages installed:\n",
        " pip install transformers sklearn pandas torch"
      ],
      "metadata": {
        "id": "i42QK2oXAZRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# Part 1:data preparation, preprocessing\n",
        "def load_and_process_test_data(test_file, tokenizer, max_length):\n",
        "    test_df = pd.read_csv(test_file)\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for _, row in test_df.iterrows():\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            str(row['Claim']) + ' [SEP] ' + str(row['Evidence']),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    return TensorDataset(input_ids, attention_masks)\n",
        "\n",
        "# Part 2:model loading\n",
        "def load_model(model_path, scaler_path, lr_path):\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=2,  # binary classification\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=True  # Enable output of hidden states for feature extraction\n",
        "    )\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    lr_classifier = joblib.load(lr_path)\n",
        "    return model, device, scaler, lr_classifier\n",
        "\n",
        "# Part 3: generating predictions using Logistic Regression\n",
        "def predict_model(model, dataloader, device, scaler, lr_classifier):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for batch in dataloader:\n",
        "        b_input_ids, b_input_mask = batch\n",
        "        b_input_ids = b_input_ids.to(device)\n",
        "        b_input_mask = b_input_mask.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_hidden_states=True)\n",
        "        embeddings = outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n",
        "        scaled_embeddings = scaler.transform(embeddings)\n",
        "        preds = lr_classifier.predict(scaled_embeddings)\n",
        "        predictions.extend(preds)\n",
        "    return predictions\n",
        "\n",
        "# Part 4: save output to file\n",
        "def save_predictions(predictions, output_file):\n",
        "    pred_df = pd.DataFrame(predictions, columns=['prediction'])\n",
        "    pred_df.to_csv(output_file, index=False)\n",
        "\n",
        "# Main execution\n",
        "def main(model_path, test_data_path, predictions_path, scaler_path, lr_path):\n",
        "    # setup tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    max_length = 256\n",
        "\n",
        "    # loading the model, scaler, and Logistic Regression classifier\n",
        "    model, device, scaler, lr_classifier = load_model(model_path, scaler_path, lr_path)\n",
        "\n",
        "    # load and process test data\n",
        "    test_dataset = load_and_process_test_data(test_data_path, tokenizer, max_length)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=SequentialSampler(test_dataset),\n",
        "        batch_size=16\n",
        "    )\n",
        "\n",
        "    # generate predictions using Logistic Regression\n",
        "    predictions = predict_model(model, test_dataloader, device, scaler, lr_classifier)\n",
        "\n",
        "    # save predictions to a file\n",
        "    save_predictions(predictions, predictions_path)\n",
        "    print(f\"Predictions saved to {predictions_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = 'bert_for_sequence_classification.pth'\n",
        "    test_data_path = 'test.csv'  # update with the path to test data\n",
        "    predictions_path = 'Group_56_C.csv'\n",
        "    scaler_path = 'scaler.joblib'\n",
        "    lr_path = 'lr_classifier.joblib'  # path to the Logistic Regression model\n",
        "    main(model_path, test_data_path, predictions_path, scaler_path, lr_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUKOz47Hs4tY",
        "outputId": "1d06fe11-2dcd-4fe9-85fa-df61a1fe5c90"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to Group_56_C.csv\n"
          ]
        }
      ]
    }
  ]
}